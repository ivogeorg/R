---
title: "pml-q2"
author: "Ivo Georgiev"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 1

Splitting a dataset into training and test subsets:

```{r p1-1}
require(caret)
require(AppliedPredictiveModeling) # package with data
data(AlzheimerDisease)
adData     <- data.frame(diagnosis, predictors)
trainIndex <- createDataPartition(diagnosis, p=0.50, list=FALSE) # returns a matrix
training   <- adData[trainIndex,]
testing    <- adData[-trainIndex,]
```


## Problem 2

Visual identification of a non-random pattern in the outcome, unexplained by the predictors:

```{r p2-1}
require(caret)
require(AppliedPredictiveModeling)
data(concrete)
set.seed(1000) # for repeatability
inTrain  <- createDataPartition(mixtures$CompressiveStrength, p=3/4)[[1]] # returns a list 
training <- mixtures[inTrain,]
testing  <- mixtures[-inTrain,]
```

Plot the outcome against the sample indices coloring each sample by each (factorized) predictor:

```{r p2-2}
require(ggplot2)
require(Hmisc)
Cut.Cement <- cut2(training$Cement)
qplot(seq_along(training$CompressiveStrength),
      training$CompressiveStrength,
      colour = Cut.Cement,
      ylab = "Compressive strength",
      xlab = "Samples")
```

```{r p2-3}
require(ggplot2)
require(Hmisc)
Cut.BlastFurnaceSlag <- cut2(training$BlastFurnaceSlag)
qplot(seq_along(training$CompressiveStrength),
      training$CompressiveStrength,
      colour = Cut.BlastFurnaceSlag,
      ylab = "Compressive strength",
      xlab = "Samples")
```

```{r p2-4}
require(ggplot2)
require(Hmisc)
Cut.FlyAsh <- cut2(training$FlyAsh)
qplot(seq_along(training$CompressiveStrength),
      training$CompressiveStrength,
      colour = Cut.FlyAsh,
      ylab = "Compressive strength",
      xlab = "Samples")
```

```{r p2-5}
require(ggplot2)
require(Hmisc)
qplot(seq_along(training$CompressiveStrength),
      training$CompressiveStrength,
      colour = cut2(training$Water),
      ylab = "Compressive strength",
      xlab = "Samples")                       # TODO: how to re-label the legend?
```

```{r p2-6}
require(ggplot2)
require(Hmisc)
qplot(seq_along(training$CompressiveStrength),
      training$CompressiveStrength,
      colour = cut2(training$Superplasticizer),
      ylab = "Compressive strength",
      xlab = "Samples")                       # TODO: how to re-label the legend?
```

```{r p2-7}
require(ggplot2)
require(Hmisc)
qplot(seq_along(training$CompressiveStrength),
      training$CompressiveStrength,
      colour = cut2(training$CoarseAggregate),
      ylab = "Compressive strength",
      xlab = "Samples")                       # TODO: how to re-label the legend?
```

```{r p2-8}
require(ggplot2)
require(Hmisc)
qplot(seq_along(training$CompressiveStrength),
      training$CompressiveStrength,
      colour = cut2(training$FineAggregate),
      ylab = "Compressive strength",
      xlab = "Samples")                       # TODO: how to re-label the legend?
```

```{r p2-9}
require(ggplot2)
require(Hmisc)
qplot(seq_along(training$CompressiveStrength),
      training$CompressiveStrength,
      colour = cut2(training$Age),
      ylab = "Compressive strength",
      xlab = "Samples")                       # TODO: how to re-label the legend?
```

There is a non-random pattern in the plot of the outcome versus sample index that does not appear to be well explained by any single predictor. _(TODO: How would it look if it were well explained by one of the predictors?)_

## Problem 3

Identifying a skewed predictor variable:

```{r p3-1}
require(caret)
require(AppliedPredictiveModeling)
data(concrete)
set.seed(1000)
inTrain=createDataPartition(mixtures$CompressiveStrength,p=3/4)[[1]] 
training=mixtures[inTrain,]
testing=mixtures[-inTrain,]
```

Plot a histogram of Superplasticizer:

```{r p3-2}
hist(training$Superplasticizer)
hist(log(training$Superplasticizer + 1)) # to avoid taking log(0)
```

The Superplasticizer predictor variable is clearly skewed. There is a large number of identical values (in this case zeroes), so even the logarithmic transformation does not result in a symmetric distribution.

## Problem 4

Using only a subset of the predictors to capture a preset fraction of the variance:

```{r p4-1}
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData   <- data.frame(diagnosis,predictors)
inTrain  <- createDataPartition(adData$diagnosis,p=3/4)[[1]]
training <- adData[ inTrain,]
testing  <- adData[-inTrain,]
```

Perform PCA pre-processing, setting the cutoff for the cumulative percent of the variance to be captured. The _preProcess_ function to will use only the necessary number of predictors during the pre-processing.

Using _grep_ to select only the IL (*i*nter*l*eukin) predictors.

```{r p4-2}
preProc1 <- preProcess(training[,grep("^IL", names(training))], method="pca", thresh = .8)
preProc1$numComp # number of PCs required to capture the specified amount of variance 
```

## Problem 5

Comparing accuracy of predictive models w/ vs w/o PCA pre-processing:

```{r p5-1}
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData1  <- data.frame(diagnosis, predictors[, grep("^IL", names(predictors))])
inTrain  <- createDataPartition(adData1$diagnosis,p=3/4)[[1]]
training <- adData1[ inTrain,]
testing  <- adData1[-inTrain,]
```

Train two models. Use a generalized linear model. Use only the interleukin predictors. The first model is w/o PCA pre-processing.

```{r p5-2}
fitNoPCA <- train(diagnosis ~ ., method="glm", data = training)
confusionMatrix(testing$diagnosis, predict(fitNoPCA, testing))
```

```{r p5-3}
preProcPCA <- preProcess(training[, -1], method = "pca", thresh = 0.8)
trainPCA   <- predict(preProcPCA, training[, -1])
fitPCA     <- train(training$diagnosis ~ ., method = "glm", data = trainPCA) # TODO: no diagnosis here
testPCA    <- predict(preProcPCA, testing[, -1])
confusionMatrix(testing$diagnosis, predict(fitPCA, testPCA))
```

The PCA preprocessing raises the accuracy from 0.65 to 0.72.
